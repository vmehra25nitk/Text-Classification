{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting necessary libraries (Run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting stop words and punctuations (Run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "temp = TweetTokenizer()\n",
    "punctuation = [' ', '!', '(', ')', '()', '-', '[', ']', '[]', '{}', '{', '}', ';', ':', '\\', ', '<', '>', '.', '/', '?', '@', '#', '$', '%', '^', '&', '*', '_', '~']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making 2 list :\n",
    "1st for training data files\n",
    "2nd for testing data files\n",
    "(Run this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting training & testing results and training & testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant Mehra\\Text Classification Project\n",
      "['20_newsgroups', 'mini_newsgroups']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_files = []\n",
    "test_files = []\n",
    "train_y = []\n",
    "test_y = []\n",
    "curDir = os.getcwd()\n",
    "print(curDir)\n",
    "direct = os.listdir(curDir)\n",
    "a = direct.index('20_newsgroups')\n",
    "b = direct.index('mini_newsgroups')\n",
    "#remove this .py file and checkpoints file\n",
    "a = direct[a]\n",
    "b = direct[b]\n",
    "direct = [a, b]\n",
    "print(direct)\n",
    "i = -1\n",
    "folderList = 0\n",
    "#go through each folder\n",
    "for fold in direct:\n",
    "    i += 1\n",
    "    path = fold\n",
    "    folderlist = os.listdir(path)\n",
    "    if \".DS_Store\" in folderlist:\n",
    "        folderlist.remove('.DS_Store')\n",
    "    if \".ipynb\" in folderlist:\n",
    "        folderlist.remove('.ipynb')\n",
    "    folderList = folderlist\n",
    "    #go through the folder list having different folders.\n",
    "    for folder in folderlist:\n",
    "        filelist = os.listdir(path + '/' + folder)\n",
    "        #go through different files of single folder.\n",
    "        for file in filelist:\n",
    "            #update path of each file and class of each file.\n",
    "            if(fold == '20_newsgroups'):\n",
    "                train_y.append(folder)\n",
    "                train_files.append((path + '/' + folder + '/' + file, i))\n",
    "            else:\n",
    "                test_y.append(folder)\n",
    "                test_files.append((path + '/' + folder + '/' + file, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using saved data to save time\n",
    "loading previously computed data into data frame and dictionary respectively (Run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"test_data_frame.txt\", 'rb')\n",
    "test_data_frame = pickle.load(pickle_in)\n",
    "pickle_in = open(\"dictionary.txt\", 'rb')\n",
    "dictionary1 = pickle.load(pickle_in)\n",
    "pickle_in = open(\"dataframe.txt\", 'rb')\n",
    "data_frame = pickle.load(pickle_in)\n",
    "pickle_in = open(\"count_class_dictn.txt\", 'rb')\n",
    "count_class_dictionary = pickle.load(pickle_in)\n",
    "count_classes_dictionary = count_class_dictionary \n",
    "pickle_in = open(\"count_class_dictn.txt\", 'rb')\n",
    "count_class_dictionary = pickle.load(pickle_in)\n",
    "count_classes_dictn = count_class_dictionary\n",
    "pickle_in = open(\"test_result.txt\", 'rb')\n",
    "test_result = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "dictionary = dictionary1\n",
    "direct = os.listdir(curDir)\n",
    "unique_train_y = set(train_y)\n",
    "train_y = pd.DataFrame(train_y)\n",
    "df = data_frame\n",
    "df_train = copy.deepcopy(df)\n",
    "df['result'] = train_y\n",
    "print(len(train_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can skip to run code to save time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating dictionary to create words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "i = 0\n",
    "#go through each file\n",
    "for file in train_files:\n",
    "    current_file = open(file[0], 'r')\n",
    "    x = current_file.readlines()\n",
    "    #go through each line\n",
    "    for line in x:\n",
    "        words = temp.tokenize(line)\n",
    "        #go through each word and build vocabulary with word count\n",
    "        for singleWord in words:\n",
    "            if(singleWord in dictionary):\n",
    "                dictionary[singleWord] += 1\n",
    "            else:\n",
    "                dictionary[singleWord] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting into form of x-y frame.\n",
    "Building dictionary of word as key and it's frequency as value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleting stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "for word in punctuation:\n",
    "    if word in dictionary:\n",
    "        del dictionary[word]\n",
    "for word in stopWords:\n",
    "    if word in dictionary:\n",
    "        del dictionary[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing words with very less count and significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = copy.deepcopy(dictionary)\n",
    "for word in new_dict.keys():\n",
    "    if(new_dict[word] < 300):\n",
    "        del dictionary[word]\n",
    "print(len((dictionary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data frame initialised before using to fit our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 2063)\n"
     ]
    }
   ],
   "source": [
    "vec = np.arange(0, len(dictionary.keys()) * len(train_files)) * 0 + 1\n",
    "df = pd.DataFrame(vec.reshape(-1, len(dictionary.keys())))\n",
    "df.columns = dictionary.keys()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting values for words in our Data Frame  making the data in x - y form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Can skip running to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#go through each file\n",
    "for file in train_files:\n",
    "    current_file = open(file[0], 'r')\n",
    "    x = current_file.readlines()\n",
    "    #go through each line\n",
    "    for line in x:\n",
    "        words = temp.tokenize(line)\n",
    "        #go through each word\n",
    "        #remove punctuations, stopWords and digits.\n",
    "        for word in punctuation:\n",
    "            if word in words:\n",
    "                words.remove(word)\n",
    "        for word in stopWords:\n",
    "            if word in dictionary:\n",
    "                words.remove(word)\n",
    "        nw = copy.deepcopy(words)\n",
    "        for word in nw:\n",
    "            if word.isdigit():\n",
    "                words.remove(word)\n",
    "        \n",
    "        #go through each word and build vocabulary with word count\n",
    "        for singleWord in words:\n",
    "            if(singleWord in dictionary):\n",
    "                if(type(singleWord) is int):\n",
    "                    continue\n",
    "                df[singleWord][i] += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Function\n",
    "This function creates a dictionary of dictionaries that has count of words in each class and total words of a class.\n",
    "Can skip to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#go through each file\n",
    "\n",
    "count_classes_dictn = {}\n",
    "for file in train_files:\n",
    "    current_class_dictn = {}\n",
    "    current_file = open(file[0], 'r')\n",
    "    x = current_file.readlines()\n",
    "    #go through each line\n",
    "    \n",
    "    for line in x:\n",
    "        words = temp.tokenize(line)\n",
    "        #go through each word\n",
    "        \n",
    "        for singleWord in words:\n",
    "            if(singleWord in current_class_dictn):\n",
    "                current_class_dictn[singleWord] += 1\n",
    "            else:\n",
    "                current_class_dictn[singleWord] = 1\n",
    "                \n",
    "    #for each file update the data frame.\n",
    "    new_dict = copy.deepcopy(current_class_dictn)\n",
    "    #remove punctuations, stopWords and other unecessary words\n",
    "    for word in punctuation:\n",
    "        if word in new_dict:\n",
    "            del current_class_dictn[word]\n",
    "            \n",
    "    new_dict = copy.deepcopy(current_class_dictn)\n",
    "    for word in stopWords:\n",
    "        if word in new_dict:\n",
    "            del current_class_dictn[word]\n",
    "            \n",
    "    new_dict = copy.deepcopy(current_class_dictn)\n",
    "    for key in new_dict:\n",
    "        if key not in dictionary:\n",
    "            del current_class_dictn[key]\n",
    "            \n",
    "    new_dict = copy.deepcopy(current_class_dictn)\n",
    "    for key in dictionary:\n",
    "        if key not in new_dict:\n",
    "            current_class_dictn[key] = 1\n",
    "    \n",
    "    #updating dictionaries for different classes\n",
    "    if train_y[0][i] not in count_classes_dictn.keys():\n",
    "        count_classes_dictn[train_y[0][i]] = {}\n",
    "    for word in current_class_dictn:\n",
    "        if word in count_classes_dictn[train_y[0][i]]:\n",
    "            count_classes_dictn[train_y[0][i]][word] += current_class_dictn[word]\n",
    "        else:\n",
    "            count_classes_dictn[train_y[0][i]][word] = current_class_dictn[word]\n",
    "            \n",
    "    #maintaining total count of words\n",
    "    if 'total_count_class_words' in count_classes_dictn[train_y[0][i]]:\n",
    "        count_classes_dictn[train_y[0][i]]['total_count_class_words'] += sum(current_class_dictn.values())\n",
    "    else:\n",
    "        count_classes_dictn[train_y[0][i]]['total_count_class_words'] = sum(current_class_dictn.values())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('count_class_dictn.txt', 'wb')\n",
    "pickle.dump(count_classes_dictn, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dictionary to predict the testing files class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through each file\n",
    "i = 0\n",
    "test_result = []\n",
    "for file in test_files:\n",
    "    current_file = open(file[0], 'r')\n",
    "    x = current_file.readlines()\n",
    "    current_dict_test = {}\n",
    "    #go through each line\n",
    "    \n",
    "    for line in x:\n",
    "        words = temp.tokenize(line)\n",
    "        #go through each word\n",
    "        \n",
    "        for singleWord in words:\n",
    "            if(singleWord in current_dict_test):\n",
    "                current_dict_test[singleWord] += 1\n",
    "            else:\n",
    "                current_dict_test[singleWord] = 1\n",
    "    \n",
    "    #deleting stop words and punctuation\n",
    "    new_dict = copy.deepcopy(current_dict_test)\n",
    "    for word in punctuation:\n",
    "        if word in new_dict.keys():\n",
    "            del current_dict_test[word]\n",
    "            \n",
    "    new_dict = copy.deepcopy(current_dict_test)\n",
    "    for word in stopWords:\n",
    "        if word in new_dict.keys():\n",
    "            del current_dict_test[word]\n",
    "            \n",
    "    new_dict = copy.deepcopy(current_dict_test)\n",
    "    for word in new_dict.keys():\n",
    "        if word not in dictionary.keys():\n",
    "            del current_dict_test[word]\n",
    "            \n",
    "    #number of words in a class c with word w\n",
    "    total_class_count = len(train_y)\n",
    "    max_proba, result_class = 0, 0\n",
    "    flag = True\n",
    "    \n",
    "    for class1 in unique_train_y:\n",
    "        df1 = df[df['result'] == class1]\n",
    "        count_of_this_class = len(df1)\n",
    "        prob_this_class = count_of_this_class / total_class_count\n",
    "        #getting probabilities of the words\n",
    "        prob_this_class = math.log(prob_this_class)\n",
    "        proba = 0\n",
    "        for word in current_dict_test:\n",
    "            proba += math.log(count_classes_dictn[class1][word]) - math.log(count_classes_dictn[class1]['total_count_class_words'])\n",
    "        proba += prob_this_class\n",
    "        if proba > max_proba:\n",
    "            max_proba = proba\n",
    "            result_class = class1\n",
    "        if flag:\n",
    "            flag = False\n",
    "            max_proba = proba\n",
    "            result_class = class1\n",
    "    test_result.append(class1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('test_result.txt', 'wb')\n",
    "pickle.dump(test_result, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gaussian naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a test data frame to be used in gaussian naive bayes (Can skip to run to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.arange(0, len(dictionary.keys()) * len(test_files)) * 0 + 1\n",
    "test_df = pd.DataFrame(vec.reshape(-1, len(dictionary.keys())))\n",
    "test_df.columns = dictionary.keys()\n",
    "i = 0\n",
    "for file in test_files:\n",
    "    current_file = open(file[0], 'r')\n",
    "    x = current_file.readlines()\n",
    "    #go through each line\n",
    "    for line in x:\n",
    "        words = temp.tokenize(line)\n",
    "        \n",
    "        for word in punctuation:\n",
    "            if word in words:\n",
    "                words.remove(word)\n",
    "        for word in stopWords:\n",
    "            if word in words:\n",
    "                words.remove(word)\n",
    "        nw = copy.deepcopy(words)\n",
    "        for word in nw:\n",
    "            if word.isdigit():\n",
    "                words.remove(word)\n",
    "        \n",
    "        #go through each word and build vocabulary with word count\n",
    "        for singleWord in words:\n",
    "            if(singleWord in dictionary):\n",
    "                if(type(singleWord) is int):\n",
    "                    print(singleWord)\n",
    "                test_df[singleWord][i] += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Vedant Mehra\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 2062)\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "if 'result' in data_frame:\n",
    "    del data_frame['result']\n",
    "clf.fit(data_frame, train_y)\n",
    "print(data_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'result' in test_data_frame:\n",
    "    del test_data_frame['result']\n",
    "y_pred_gaussian = clf.predict(test_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.68      0.97      0.80       100\n",
      "           comp.graphics       0.75      0.52      0.62       100\n",
      " comp.os.ms-windows.misc       0.72      0.52      0.60       100\n",
      "comp.sys.ibm.pc.hardware       0.49      0.82      0.61       100\n",
      "   comp.sys.mac.hardware       0.90      0.78      0.83       100\n",
      "          comp.windows.x       0.90      0.72      0.80       100\n",
      "            misc.forsale       0.62      0.95      0.75       100\n",
      "               rec.autos       0.92      0.78      0.84       100\n",
      "         rec.motorcycles       0.99      0.97      0.98       100\n",
      "      rec.sport.baseball       0.99      1.00      1.00       100\n",
      "        rec.sport.hockey       1.00      0.99      0.99       100\n",
      "               sci.crypt       0.99      0.99      0.99       100\n",
      "         sci.electronics       0.76      0.71      0.74       100\n",
      "                 sci.med       0.99      0.94      0.96       100\n",
      "               sci.space       0.95      0.99      0.97       100\n",
      "  soc.religion.christian       1.00      0.99      0.99       100\n",
      "      talk.politics.guns       0.74      0.84      0.79       100\n",
      "   talk.politics.mideast       0.97      0.91      0.94       100\n",
      "      talk.politics.misc       0.81      0.46      0.59       100\n",
      "      talk.religion.misc       0.57      0.51      0.54       100\n",
      "\n",
      "             avg / total       0.84      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, y_pred_gaussian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
